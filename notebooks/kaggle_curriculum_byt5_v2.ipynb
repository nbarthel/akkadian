{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-00",
   "metadata": {},
   "source": [
    "# Deep Past Challenge — Curriculum ByT5-Base v2\n",
    "\n",
    "**Model**: `notninja/byt5-base-akkadian` (warm-start, already fine-tuned on general Akkadian)\n",
    "\n",
    "**Strategy**: Skip Phase 1 entirely — warm-start model already covers general Akkadian.\n",
    "- Phase 2: Old Assyrian specialization (~15K samples, 10 epochs)\n",
    "- Phase 3 (optional): Old Assyrian + remaining lexicon entries (2 epochs)\n",
    "\n",
    "**Key improvements over v1**:\n",
    "- Warm-start from `notninja/byt5-base-akkadian` instead of `google/byt5-small`\n",
    "- MAX_SOURCE_LENGTH = 768 (covers 97%+ of competition val inputs at byte level)\n",
    "- Zero-shot baseline eval before training\n",
    "- Decoding hyperparameter search (length_penalty, num_beams) on competition val\n",
    "- fp16=True for T4 tensor core acceleration\n",
    "\n",
    "**Metric**: sqrt(BLEU * chrF++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"fp16 supported: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-03",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "source": "# Paths — ASSEMBLED_DIR and COMPETITION_DIR are auto-detected in the next cell\nOUTPUT_DIR = Path('/kaggle/working')\n\n# Model — warm-start: already fine-tuned on general Akkadian, skip Phase 1\nMODEL_NAME = 'notninja/byt5-base-akkadian'\nPREFIX = 'translate Akkadian to English: '\n\n# Sequence lengths (ByT5 byte-level).\n# Competition val inputs average 435 chars, max ~921 chars.\n# With UTF-8 diacritics each char can be 1-2 bytes; 768 bytes covers ~97% of inputs.\nMAX_SOURCE_LENGTH = 768\nMAX_TARGET_LENGTH = 512\n\n# Phase 2: Old Assyrian specialization\nP2_EPOCHS = 10\nP2_BATCH_SIZE = 8\nP2_GRAD_ACCUM = 4\nP2_LR = 1e-5\nP2_WARMUP = 0.05\n\n# Phase 3: Old Assyrian + lexicon (optional)\nP3_EPOCHS = 2\nP3_BATCH_SIZE = 8\nP3_GRAD_ACCUM = 4\nP3_LR = 5e-6\nP3_WARMUP = 0.05\n\nprint(\"Configuration:\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  MAX_SOURCE_LENGTH: {MAX_SOURCE_LENGTH}\")\nprint(f\"  MAX_TARGET_LENGTH: {MAX_TARGET_LENGTH}\")\nprint(f\"  Phase 2: {P2_EPOCHS} epochs, batch={P2_BATCH_SIZE}, grad_accum={P2_GRAD_ACCUM}, lr={P2_LR}\")\nprint(f\"  Phase 3: {P3_EPOCHS} epochs, batch={P3_BATCH_SIZE}, grad_accum={P3_GRAD_ACCUM}, lr={P3_LR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-05",
   "metadata": {},
   "source": "## Load Data"
  },
  {
   "cell_type": "code",
   "id": "hmkbuc4wp18",
   "source": "import os\n\n# Diagnostic: show what's available in /kaggle/input/\nprint(\"Contents of /kaggle/input/:\")\nif os.path.exists('/kaggle/input'):\n    for d in sorted(os.listdir('/kaggle/input')):\n        full = os.path.join('/kaggle/input', d)\n        if os.path.isdir(full):\n            print(f\"  {d}/\")\n            for f2 in sorted(os.listdir(full)):\n                sub = os.path.join(full, f2)\n                if os.path.isdir(sub):\n                    print(f\"    {f2}/\")\n                    for f3 in sorted(os.listdir(sub))[:10]:\n                        sub2 = os.path.join(sub, f3)\n                        if os.path.isdir(sub2):\n                            items = os.listdir(sub2)\n                            print(f\"      {f3}/ ({len(items)} items): {items[:8]}\")\n                        else:\n                            print(f\"      {f3}  ({os.path.getsize(sub2)} bytes)\")\n                else:\n                    print(f\"    {f2}  ({os.path.getsize(sub)} bytes)\")\n        else:\n            print(f\"  {d}  ({os.path.getsize(full)} bytes)\")\nelse:\n    print(\"  /kaggle/input does not exist!\")\n\n# Auto-detect assembled dataset path\ndef find_file(filename, base='/kaggle/input'):\n    \"\"\"Recursively find a file under base dir.\"\"\"\n    for root, dirs, files in os.walk(base):\n        if filename in files:\n            return Path(root)\n    return None\n\nASSEMBLED_DIR = find_file('train.parquet')\nif ASSEMBLED_DIR is None:\n    raise FileNotFoundError(\"Cannot find train.parquet under /kaggle/input/\")\nprint(f\"\\nAssembled data at: {ASSEMBLED_DIR}\")\nprint(f\"  Files: {list(ASSEMBLED_DIR.iterdir())}\")\n\n# Auto-detect competition data path\nCOMPETITION_DIR = find_file('test.csv')\nif COMPETITION_DIR is None:\n    raise FileNotFoundError(\"Cannot find test.csv under /kaggle/input/\")\nprint(f\"Competition data at: {COMPETITION_DIR}\")\nprint(f\"  Files: {list(COMPETITION_DIR.iterdir())}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembled dataset (from akkadian-assembled-161k)\n",
    "train_df = pd.read_parquet(ASSEMBLED_DIR / 'train.parquet')\n",
    "val_df = pd.read_parquet(ASSEMBLED_DIR / 'val.parquet')\n",
    "comp_df = pd.read_parquet(ASSEMBLED_DIR / 'val_competition.parquet')\n",
    "\n",
    "# Competition test set (4 rows to predict)\n",
    "test_df = pd.read_csv(COMPETITION_DIR / 'test.csv')\n",
    "\n",
    "print(f\"Full train: {len(train_df):,}\")\n",
    "print(f\"Val: {len(val_df):,}\")\n",
    "print(f\"Competition val: {len(comp_df)}\")\n",
    "print(f\"Test: {len(test_df)}\")\n",
    "print(f\"\\nDialects in train: {train_df['dialect'].value_counts().to_dict()}\")\n",
    "print(f\"Quality in train: {train_df['quality'].value_counts().to_dict()}\")\n",
    "\n",
    "# Phase 2 filter: Old Assyrian dialect\n",
    "p2_train = train_df[train_df['dialect'] == 'old_assyrian'].reset_index(drop=True)\n",
    "print(f\"\\nPhase 2 (Old Assyrian) samples: {len(p2_train):,}\")\n",
    "\n",
    "# Phase 3 filter: Old Assyrian + non-OA lexicon entries\n",
    "oa_mask = train_df['dialect'] == 'old_assyrian'\n",
    "lex_mask = (train_df['quality'] == 'lexicon') & (~oa_mask)\n",
    "p3_train = pd.concat([train_df[oa_mask], train_df[lex_mask]], ignore_index=True)\n",
    "print(f\"Phase 3 (OA + lexicon) samples: {len(p3_train):,}\")\n",
    "\n",
    "# Quick look at competition val input lengths (byte-level)\n",
    "comp_df['src_bytes'] = comp_df['transliteration'].apply(lambda x: len((PREFIX + str(x)).encode('utf-8')))\n",
    "print(f\"\\nCompetition val source byte lengths:\")\n",
    "print(f\"  mean={comp_df['src_bytes'].mean():.0f}, median={comp_df['src_bytes'].median():.0f}, \"\n",
    "      f\"max={comp_df['src_bytes'].max()}, p95={comp_df['src_bytes'].quantile(0.95):.0f}\")\n",
    "print(f\"  Covered by MAX_SOURCE_LENGTH={MAX_SOURCE_LENGTH}: \"\n",
    "      f\"{(comp_df['src_bytes'] <= MAX_SOURCE_LENGTH).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-07",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, prefix, max_source_length, max_target_length):\n",
    "    \"\"\"Tokenize inputs and targets with separate max lengths.\"\"\"\n",
    "    inputs = [prefix + str(text) for text in examples['transliteration']]\n",
    "    targets = [str(text) for text in examples['translation']]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def score_predictions(predictions, references, prefix=\"\"):\n",
    "    \"\"\"Compute BLEU, chrF++, and geo_mean.\"\"\"\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF(word_order=2)\n",
    "    bleu_score = bleu.corpus_score(predictions, [references]).score\n",
    "    chrf_score = chrf.corpus_score(predictions, [references]).score\n",
    "    geo_mean = np.sqrt(max(bleu_score, 0) * max(chrf_score, 0))\n",
    "    p = f\"{prefix}_\" if prefix else \"\"\n",
    "    return {f\"{p}bleu\": bleu_score, f\"{p}chrf\": chrf_score, f\"{p}geo_mean\": geo_mean}\n",
    "\n",
    "\n",
    "def create_compute_metrics(tokenizer):\n",
    "    \"\"\"Create metrics computation function for Trainer.\"\"\"\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF(word_order=2)\n",
    "\n",
    "    def compute_metrics(predictions_and_labels):\n",
    "        preds, labels = predictions_and_labels\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        b = bleu.corpus_score(decoded_preds, [decoded_labels]).score\n",
    "        c = chrf.corpus_score(decoded_preds, [decoded_labels]).score\n",
    "        return {'bleu': b, 'chrf': c, 'geo_mean': np.sqrt(max(b, 0) * max(c, 0))}\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "class FullValCallback(TrainerCallback):\n",
    "    \"\"\"Score predictions on the full 8K validation set after each eval.\"\"\"\n",
    "    def __init__(self, trainer_ref, full_val_dataset, full_val_refs, tokenizer):\n",
    "        self.trainer_ref = trainer_ref\n",
    "        self.full_val_dataset = full_val_dataset\n",
    "        self.full_val_refs = full_val_refs\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        preds = self.trainer_ref.predict(self.full_val_dataset)\n",
    "        decoded = self.tokenizer.batch_decode(preds.predictions, skip_special_tokens=True)\n",
    "        metrics = score_predictions(decoded, self.full_val_refs, prefix=\"full_val\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "        state.log_history[-1].update(metrics)\n",
    "\n",
    "\n",
    "def generate_predictions(model, tokenizer, texts, prefix, max_source_length,\n",
    "                          max_target_length, num_beams=5, length_penalty=1.0,\n",
    "                          no_repeat_ngram_size=3, batch_size=8):\n",
    "    \"\"\"Generate translations in batches. Returns list of decoded strings.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    inputs = [prefix + str(t) for t in texts]\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch = inputs[i:i + batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            max_length=max_source_length,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                input_ids=enc['input_ids'],\n",
    "                attention_mask=enc['attention_mask'],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=length_penalty,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        decoded = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "        all_preds.extend(decoded)\n",
    "    return all_preds\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-09",
   "metadata": {},
   "source": [
    "## Zero-Shot Eval — Warm-Start Baseline\n",
    "\n",
    "Before any fine-tuning, evaluate `notninja/byt5-base-akkadian` on the competition val set to establish a pre-training baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading warm-start model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_zs = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model_zs.to(device)\n",
    "print(f\"Parameters: {model_zs.num_parameters():,}\")\n",
    "\n",
    "# Zero-shot predictions on competition val (88 rows)\n",
    "print(\"\\nRunning zero-shot predictions on competition val...\")\n",
    "zs_preds = generate_predictions(\n",
    "    model_zs, tokenizer,\n",
    "    comp_df['transliteration'].tolist(),\n",
    "    PREFIX, MAX_SOURCE_LENGTH, MAX_TARGET_LENGTH,\n",
    "    num_beams=5, length_penalty=1.0,\n",
    ")\n",
    "comp_refs = comp_df['translation'].tolist()\n",
    "zs_metrics = score_predictions(zs_preds, comp_refs, prefix=\"zeroshot\")\n",
    "\n",
    "print(\"\\nZero-shot baseline (competition val):\")\n",
    "for k, v in zs_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nSample zero-shot translations:\")\n",
    "for i in range(min(3, len(zs_preds))):\n",
    "    print(f\"\\n--- Sample {i} ---\")\n",
    "    print(f\"Source: {comp_df['transliteration'].iloc[i][:120]}\")\n",
    "    print(f\"Pred:   {zs_preds[i][:200]}\")\n",
    "    print(f\"Ref:    {comp_refs[i][:200]}\")\n",
    "\n",
    "# Store zero-shot metrics for summary\n",
    "baseline_zeroshot = zs_metrics.copy()\n",
    "\n",
    "# Free memory before training\n",
    "del model_zs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nModel freed from GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Phase 2: Old Assyrian Specialization\n",
    "\n",
    "Fine-tune the warm-start model on ~15K Old Assyrian dialect pairs. This is the competition domain.\n",
    "Estimated time: 2-3 hours on T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Phase 2 training samples: {len(p2_train):,}\")\n",
    "print(f\"Competition val samples: {len(comp_df)}\")\n",
    "print(f\"Full val samples: {len(val_df):,}\")\n",
    "\n",
    "p2_checkpoint_dir = OUTPUT_DIR / 'phase2_checkpoints'\n",
    "p2_best_dir = OUTPUT_DIR / 'phase2_best'\n",
    "\n",
    "# Load model fresh from warm-start checkpoint\n",
    "print(f\"\\nLoading model from: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_p2 = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model_p2.to(device)\n",
    "print(f\"Parameters: {model_p2.num_parameters():,}\")\n",
    "\n",
    "# Build HuggingFace datasets\n",
    "train_data_p2 = p2_train[['transliteration', 'translation']].reset_index(drop=True)\n",
    "comp_data = comp_df[['transliteration', 'translation']].reset_index(drop=True)\n",
    "full_val_data = val_df[['transliteration', 'translation']].reset_index(drop=True)\n",
    "full_val_refs = full_val_data['translation'].tolist()\n",
    "\n",
    "preprocess_fn = lambda x: preprocess_function(\n",
    "    x, tokenizer, PREFIX, MAX_SOURCE_LENGTH, MAX_TARGET_LENGTH\n",
    ")\n",
    "remove_cols = ['transliteration', 'translation']\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_dataset_p2 = HFDataset.from_pandas(train_data_p2).map(\n",
    "    preprocess_fn, batched=True, remove_columns=remove_cols\n",
    ")\n",
    "eval_dataset_p2 = HFDataset.from_pandas(comp_data).map(\n",
    "    preprocess_fn, batched=True, remove_columns=remove_cols\n",
    ")\n",
    "full_val_dataset_p2 = HFDataset.from_pandas(full_val_data).map(\n",
    "    preprocess_fn, batched=True, remove_columns=remove_cols\n",
    ")\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# Training arguments\n",
    "training_args_p2 = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(p2_checkpoint_dir),\n",
    "    save_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=P2_LR,\n",
    "    per_device_train_batch_size=P2_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=P2_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=P2_GRAD_ACCUM,\n",
    "    num_train_epochs=P2_EPOCHS,\n",
    "    warmup_ratio=P2_WARMUP,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    fp16=torch.cuda.is_available(),   # T4 tensor cores: use fp16, not bf16\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='geo_mean',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "data_collator_p2 = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, model=model_p2, padding=True, label_pad_token_id=-100\n",
    ")\n",
    "\n",
    "trainer_p2 = Seq2SeqTrainer(\n",
    "    model=model_p2,\n",
    "    args=training_args_p2,\n",
    "    train_dataset=train_dataset_p2,\n",
    "    eval_dataset=eval_dataset_p2,\n",
    "    data_collator=data_collator_p2,\n",
    "    compute_metrics=create_compute_metrics(tokenizer),\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer_p2.add_callback(\n",
    "    FullValCallback(trainer_p2, full_val_dataset_p2, full_val_refs, tokenizer)\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Phase 2 training...\")\n",
    "trainer_p2.train()\n",
    "\n",
    "print(\"\\nFinal Phase 2 eval on competition val:\")\n",
    "p2_results = trainer_p2.evaluate()\n",
    "for k, v in p2_results.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "# Save best checkpoint\n",
    "model_p2.save_pretrained(p2_best_dir)\n",
    "tokenizer.save_pretrained(p2_best_dir)\n",
    "print(f\"\\nPhase 2 best model saved to: {p2_best_dir}\")\n",
    "\n",
    "# Clean up intermediate checkpoints\n",
    "if p2_checkpoint_dir.exists():\n",
    "    shutil.rmtree(p2_checkpoint_dir)\n",
    "    print(f\"Cleaned up checkpoints: {p2_checkpoint_dir}\")\n",
    "\n",
    "# Store metrics for summary\n",
    "p2_competition_metrics = {\n",
    "    'bleu': p2_results.get('eval_bleu', float('nan')),\n",
    "    'chrf': p2_results.get('eval_chrf', float('nan')),\n",
    "    'geo_mean': p2_results.get('eval_geo_mean', float('nan')),\n",
    "}\n",
    "\n",
    "# Free GPU memory before optional Phase 3\n",
    "del model_p2\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Phase 2 complete. GPU memory freed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Phase 3 (Optional): Old Assyrian + Lexicon\n",
    "\n",
    "Continue from Phase 2 best, training on Old Assyrian + remaining lexicon entries for 2 epochs at a lower LR.\n",
    "This may improve rare-word coverage. **Skip this cell if Phase 2 results are already good or disk space is tight.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_PHASE3 = True   # Set to False to skip Phase 3\n",
    "\n",
    "p3_results = None\n",
    "p3_competition_metrics = None\n",
    "p3_best_dir = OUTPUT_DIR / 'phase3_best'\n",
    "FINAL_MODEL_DIR = p2_best_dir  # Default: use Phase 2 unless Phase 3 improves\n",
    "\n",
    "if RUN_PHASE3:\n",
    "    print(f\"Phase 3 training samples: {len(p3_train):,}\")\n",
    "    p3_checkpoint_dir = OUTPUT_DIR / 'phase3_checkpoints'\n",
    "\n",
    "    # Resume from Phase 2 best\n",
    "    print(f\"\\nLoading Phase 2 best model from: {p2_best_dir}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(p2_best_dir)\n",
    "    model_p3 = AutoModelForSeq2SeqLM.from_pretrained(p2_best_dir)\n",
    "    model_p3.to(device)\n",
    "\n",
    "    train_data_p3 = p3_train[['transliteration', 'translation']].reset_index(drop=True)\n",
    "    preprocess_fn3 = lambda x: preprocess_function(\n",
    "        x, tokenizer, PREFIX, MAX_SOURCE_LENGTH, MAX_TARGET_LENGTH\n",
    "    )\n",
    "\n",
    "    print(\"Tokenizing Phase 3 datasets...\")\n",
    "    train_dataset_p3 = HFDataset.from_pandas(train_data_p3).map(\n",
    "        preprocess_fn3, batched=True, remove_columns=remove_cols\n",
    "    )\n",
    "    eval_dataset_p3 = HFDataset.from_pandas(comp_data).map(\n",
    "        preprocess_fn3, batched=True, remove_columns=remove_cols\n",
    "    )\n",
    "    full_val_dataset_p3 = HFDataset.from_pandas(full_val_data).map(\n",
    "        preprocess_fn3, batched=True, remove_columns=remove_cols\n",
    "    )\n",
    "\n",
    "    training_args_p3 = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(p3_checkpoint_dir),\n",
    "        save_strategy='epoch',\n",
    "        eval_strategy='epoch',\n",
    "        learning_rate=P3_LR,\n",
    "        per_device_train_batch_size=P3_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=P3_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=P3_GRAD_ACCUM,\n",
    "        num_train_epochs=P3_EPOCHS,\n",
    "        warmup_ratio=P3_WARMUP,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=MAX_TARGET_LENGTH,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='geo_mean',\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        report_to='none',\n",
    "    )\n",
    "\n",
    "    data_collator_p3 = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer, model=model_p3, padding=True, label_pad_token_id=-100\n",
    "    )\n",
    "\n",
    "    trainer_p3 = Seq2SeqTrainer(\n",
    "        model=model_p3,\n",
    "        args=training_args_p3,\n",
    "        train_dataset=train_dataset_p3,\n",
    "        eval_dataset=eval_dataset_p3,\n",
    "        data_collator=data_collator_p3,\n",
    "        compute_metrics=create_compute_metrics(tokenizer),\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "    trainer_p3.add_callback(\n",
    "        FullValCallback(trainer_p3, full_val_dataset_p3, full_val_refs, tokenizer)\n",
    "    )\n",
    "\n",
    "    print(\"\\nStarting Phase 3 training...\")\n",
    "    trainer_p3.train()\n",
    "\n",
    "    print(\"\\nFinal Phase 3 eval on competition val:\")\n",
    "    p3_results = trainer_p3.evaluate()\n",
    "    for k, v in p3_results.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "    p3_competition_metrics = {\n",
    "        'bleu': p3_results.get('eval_bleu', float('nan')),\n",
    "        'chrf': p3_results.get('eval_chrf', float('nan')),\n",
    "        'geo_mean': p3_results.get('eval_geo_mean', float('nan')),\n",
    "    }\n",
    "\n",
    "    # Save Phase 3 best\n",
    "    model_p3.save_pretrained(p3_best_dir)\n",
    "    tokenizer.save_pretrained(p3_best_dir)\n",
    "    print(f\"Phase 3 best model saved to: {p3_best_dir}\")\n",
    "\n",
    "    if p3_checkpoint_dir.exists():\n",
    "        shutil.rmtree(p3_checkpoint_dir)\n",
    "        print(f\"Cleaned up Phase 3 checkpoints.\")\n",
    "\n",
    "    # Choose best final model based on competition geo_mean\n",
    "    p2_geo = p2_competition_metrics['geo_mean']\n",
    "    p3_geo = p3_competition_metrics['geo_mean']\n",
    "    if p3_geo >= p2_geo:\n",
    "        FINAL_MODEL_DIR = p3_best_dir\n",
    "        print(f\"\\nPhase 3 better ({p3_geo:.4f} >= {p2_geo:.4f}). Using Phase 3 for submission.\")\n",
    "    else:\n",
    "        FINAL_MODEL_DIR = p2_best_dir\n",
    "        print(f\"\\nPhase 2 better ({p2_geo:.4f} > {p3_geo:.4f}). Using Phase 2 for submission.\")\n",
    "\n",
    "    del model_p3\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Phase 3 complete. GPU memory freed.\")\n",
    "else:\n",
    "    print(\"Phase 3 skipped. Using Phase 2 model for submission.\")\n",
    "\n",
    "print(f\"\\nFinal model directory: {FINAL_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Decoding Hyperparameter Search\n",
    "\n",
    "Try combinations of `length_penalty` and `num_beams` on competition val to find the best decoding config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading final model from: {FINAL_MODEL_DIR}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINAL_MODEL_DIR)\n",
    "model_final = AutoModelForSeq2SeqLM.from_pretrained(FINAL_MODEL_DIR)\n",
    "model_final.to(device)\n",
    "print(f\"Model loaded ({model_final.num_parameters():,} parameters)\")\n",
    "\n",
    "comp_trans = comp_df['transliteration'].tolist()\n",
    "comp_refs_list = comp_df['translation'].tolist()\n",
    "\n",
    "# Grid search over length_penalty and num_beams\n",
    "length_penalties = [0.6, 0.8, 1.0, 1.2]\n",
    "beam_sizes = [5, 8]\n",
    "\n",
    "decode_results = []\n",
    "\n",
    "print(\"\\nDecoding hyperparameter search on competition val:\")\n",
    "print(f\"{'length_penalty':>15} {'num_beams':>10} {'BLEU':>8} {'chrF++':>8} {'geo_mean':>10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "best_geo = -1.0\n",
    "best_config = {'length_penalty': 1.0, 'num_beams': 5}\n",
    "\n",
    "for lp in length_penalties:\n",
    "    for nb in beam_sizes:\n",
    "        preds = generate_predictions(\n",
    "            model_final, tokenizer,\n",
    "            comp_trans, PREFIX,\n",
    "            MAX_SOURCE_LENGTH, MAX_TARGET_LENGTH,\n",
    "            num_beams=nb,\n",
    "            length_penalty=lp,\n",
    "            no_repeat_ngram_size=3,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        m = score_predictions(preds, comp_refs_list)\n",
    "        decode_results.append({\n",
    "            'length_penalty': lp,\n",
    "            'num_beams': nb,\n",
    "            **m,\n",
    "        })\n",
    "        print(f\"{lp:>15.1f} {nb:>10} {m['bleu']:>8.2f} {m['chrf']:>8.2f} {m['geo_mean']:>10.4f}\")\n",
    "        if m['geo_mean'] > best_geo:\n",
    "            best_geo = m['geo_mean']\n",
    "            best_config = {'length_penalty': lp, 'num_beams': nb}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(f\"Best config: length_penalty={best_config['length_penalty']}, num_beams={best_config['num_beams']}\")\n",
    "print(f\"Best competition geo_mean: {best_geo:.4f}\")\n",
    "\n",
    "# Store decode results as DataFrame\n",
    "decode_df = pd.DataFrame(decode_results)\n",
    "print(\"\\nFull decode search results:\")\n",
    "print(decode_df.sort_values('geo_mean', ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Generate Test Predictions\n",
    "\n",
    "Use the best decoding config found above to generate predictions on the 4 competition test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating test predictions with best config:\")\n",
    "print(f\"  num_beams={best_config['num_beams']}, length_penalty={best_config['length_penalty']}\")\n",
    "print(f\"  MAX_SOURCE_LENGTH={MAX_SOURCE_LENGTH}, MAX_TARGET_LENGTH={MAX_TARGET_LENGTH}\")\n",
    "\n",
    "test_preds = generate_predictions(\n",
    "    model_final, tokenizer,\n",
    "    test_df['transliteration'].tolist(),\n",
    "    PREFIX,\n",
    "    MAX_SOURCE_LENGTH, MAX_TARGET_LENGTH,\n",
    "    num_beams=best_config['num_beams'],\n",
    "    length_penalty=best_config['length_penalty'],\n",
    "    no_repeat_ngram_size=3,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "print(\"\\nTest predictions:\")\n",
    "for i, (src, pred) in enumerate(zip(test_df['transliteration'], test_preds)):\n",
    "    print(f\"\\n=== Sample {i} (id={test_df['id'].iloc[i]}) ===\")\n",
    "    print(f\"Source:      {str(src)[:150]}\")\n",
    "    print(f\"Translation: {pred[:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test_df['id'], 'translation': test_preds})\n",
    "submission_path = OUTPUT_DIR / 'submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SUMMARY — Deep Past Challenge v2\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"MAX_SOURCE_LENGTH: {MAX_SOURCE_LENGTH}  MAX_TARGET_LENGTH: {MAX_TARGET_LENGTH}\")\n",
    "print()\n",
    "\n",
    "# Zero-shot baseline\n",
    "if 'baseline_zeroshot' in dir():\n",
    "    zs = baseline_zeroshot\n",
    "    print(\"Zero-shot (competition val — before fine-tuning):\")\n",
    "    print(f\"  BLEU={zs['zeroshot_bleu']:.2f}  chrF++={zs['zeroshot_chrf']:.2f}  geo_mean={zs['zeroshot_geo_mean']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Phase 2 results\n",
    "if 'p2_competition_metrics' in dir() and p2_competition_metrics:\n",
    "    m = p2_competition_metrics\n",
    "    print(f\"Phase 2 — Old Assyrian ({P2_EPOCHS} epochs, lr={P2_LR}):\")\n",
    "    print(f\"  BLEU={m['bleu']:.2f}  chrF++={m['chrf']:.2f}  geo_mean={m['geo_mean']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Phase 3 results (if run)\n",
    "if p3_competition_metrics is not None:\n",
    "    m = p3_competition_metrics\n",
    "    print(f\"Phase 3 — OA + Lexicon ({P3_EPOCHS} epochs, lr={P3_LR}):\")\n",
    "    print(f\"  BLEU={m['bleu']:.2f}  chrF++={m['chrf']:.2f}  geo_mean={m['geo_mean']:.4f}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"Phase 3: skipped\")\n",
    "    print()\n",
    "\n",
    "# Best decoding config\n",
    "if 'best_config' in dir() and 'best_geo' in dir():\n",
    "    print(f\"Best decoding config: num_beams={best_config['num_beams']}, length_penalty={best_config['length_penalty']}\")\n",
    "    print(f\"Best competition geo_mean (post-decode search): {best_geo:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Final model used\n",
    "print(f\"Final model: {FINAL_MODEL_DIR}\")\n",
    "print(f\"Submission: {submission_path}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}