{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: ByT5 Training with NLLB-Augmented Data\n",
    "\n",
    "Fine-tunes `notninja/byt5-base-akkadian` on gold translations + NLLB teacher translations.\n",
    "\n",
    "**Requires:** Output from Notebook 1 (`gold_with_nllb.parquet`) uploaded as Kaggle dataset `nicbarthelemy1/akkadian-nllb-translations`.\n",
    "\n",
    "**Training plan:**\n",
    "- Phase 1: All augmented gold data (~252K rows: 126K gold + 126K NLLB)\n",
    "- Phase 2: Old Assyrian only (competition domain specialization)\n",
    "- Checkpoint selection on competition val (95 OA samples) by geo_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sacrebleu datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, gc, math, time, warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import (\n    AutoTokenizer, AutoModelForSeq2SeqLM,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq,\n)\nfrom datasets import Dataset\nfrom sacrebleu.metrics import BLEU, CHRF\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file(filename, base='/kaggle/input'):\n",
    "    for root, dirs, files in os.walk(base):\n",
    "        if filename in files:\n",
    "            return Path(root)\n",
    "    return None\n",
    "\n",
    "# Show input layout\n",
    "print('Contents of /kaggle/input/')\n",
    "for d in sorted(os.listdir('/kaggle/input')):\n",
    "    full = os.path.join('/kaggle/input', d)\n",
    "    if os.path.isdir(full):\n",
    "        print(f'  {d}/')\n",
    "        for f in sorted(os.listdir(full))[:15]:\n",
    "            print(f'    {f}')\n",
    "\n",
    "# Load NLLB-augmented gold data (from Notebook 1 output)\n",
    "NLLB_DIR = find_file('gold_with_nllb.parquet')\n",
    "if NLLB_DIR is None:\n",
    "    raise FileNotFoundError(\n",
    "        'Cannot find gold_with_nllb.parquet. '\n",
    "        'Run Notebook 1 (nllb_distill) first, then upload its output as a Kaggle dataset.'\n",
    "    )\n",
    "print(f'\\nNLLB data at: {NLLB_DIR}')\n",
    "gold_nllb_df = pd.read_parquet(NLLB_DIR / 'gold_with_nllb.parquet')\n",
    "\n",
    "# Load original assembled data (for val splits)\n",
    "ASSEMBLED_DIR = find_file('val_competition.parquet')\n",
    "val_comp = pd.read_parquet(ASSEMBLED_DIR / 'val_competition.parquet')\n",
    "\n",
    "# Competition test data\n",
    "COMP_DIR = find_file('test.csv')\n",
    "test_df = pd.read_csv(COMP_DIR / 'test.csv') if COMP_DIR else None\n",
    "\n",
    "# Also load NLLB test predictions if available (for ensembling)\n",
    "nllb_test_path = find_file('test_nllb_predictions.csv')\n",
    "nllb_test_df = pd.read_csv(nllb_test_path / 'test_nllb_predictions.csv') if nllb_test_path else None\n",
    "\n",
    "print(f'Gold+NLLB training: {len(gold_nllb_df)}')\n",
    "print(f'Competition val: {len(val_comp)}')\n",
    "if test_df is not None:\n",
    "    print(f'Test samples: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Augmented Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original gold pairs\n",
    "orig = gold_nllb_df[['transliteration', 'translation', 'dialect']].copy()\n",
    "orig['source_type'] = 'gold'\n",
    "\n",
    "# NLLB-generated pairs (filter out empty translations)\n",
    "nllb = gold_nllb_df[['transliteration', 'nllb_translation', 'dialect']].copy()\n",
    "nllb = nllb.rename(columns={'nllb_translation': 'translation'})\n",
    "nllb = nllb[nllb['translation'].str.len() > 0].reset_index(drop=True)\n",
    "nllb['source_type'] = 'nllb'\n",
    "\n",
    "# Combine with equal weight\n",
    "augmented = pd.concat([orig, nllb], ignore_index=True)\n",
    "augmented = augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f'Augmented training: {len(augmented)} rows ({len(orig)} gold + {len(nllb)} nllb)')\n",
    "\n",
    "# Old Assyrian subsets for Phase 2\n",
    "oa_augmented = augmented[augmented['dialect'] == 'old_assyrian'].reset_index(drop=True)\n",
    "print(f'OA augmented: {len(oa_augmented)} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ByT5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BYT5_MODEL = 'notninja/byt5-base-akkadian'\n",
    "PREFIX = 'translate Akkadian to English: '\n",
    "MAX_SOURCE = 768\n",
    "MAX_TARGET = 512\n",
    "\n",
    "print(f'Loading {BYT5_MODEL}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(BYT5_MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(BYT5_MODEL).to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f'Loaded ({n_params:.0f}M params)')\n",
    "print(f'GPU memory: {torch.cuda.memory_allocated()/1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    inputs = [PREFIX + str(t) for t in examples['transliteration']]\n",
    "    targets = [str(t) for t in examples['translation']]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_SOURCE, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=MAX_TARGET, truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "def score_preds(preds, refs):\n",
    "    b = BLEU().corpus_score(preds, [refs]).score\n",
    "    c = CHRF(word_order=2).corpus_score(preds, [refs]).score\n",
    "    g = math.sqrt(max(b, 0) * max(c, 0))\n",
    "    return {'bleu': b, 'chrf': c, 'geo_mean': g}\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return score_preds(decoded_preds, decoded_labels)\n",
    "\n",
    "# Prepare eval dataset (competition val â€” used for all phases)\n",
    "eval_ds = Dataset.from_pandas(val_comp[['transliteration', 'translation']])\n",
    "eval_ds = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "print(f'Eval dataset: {len(eval_ds)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: All Augmented Gold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'=== Phase 1: All augmented gold ({len(augmented)} samples) ===')\n",
    "\n",
    "train_ds = Dataset.from_pandas(augmented[['transliteration', 'translation']])\n",
    "train_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "\n",
    "phase1_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='/kaggle/working/byt5-nllb-phase1',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=2000,\n",
    "    save_strategy='steps',\n",
    "    save_steps=2000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='geo_mean',\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET,\n",
    "    generation_num_beams=4,\n",
    "    report_to='none',\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=phase1_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('/kaggle/working/byt5-nllb-phase1-best')\n",
    "tokenizer.save_pretrained('/kaggle/working/byt5-nllb-phase1-best')\n",
    "print('Phase 1 complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Old Assyrian Specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n=== Phase 2: Old Assyrian ({len(oa_augmented)} samples) ===')\n",
    "\n",
    "oa_train_ds = Dataset.from_pandas(oa_augmented[['transliteration', 'translation']])\n",
    "oa_train_ds = oa_train_ds.map(preprocess, batched=True, remove_columns=oa_train_ds.column_names)\n",
    "\n",
    "phase2_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='/kaggle/working/byt5-nllb-phase2',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='geo_mean',\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET,\n",
    "    generation_num_beams=4,\n",
    "    report_to='none',\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=phase2_args,\n",
    "    train_dataset=oa_train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('/kaggle/working/byt5-nllb-phase2-best')\n",
    "tokenizer.save_pretrained('/kaggle/working/byt5-nllb-phase2-best')\n",
    "print('Phase 2 complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_trans = val_comp['transliteration'].tolist()\n",
    "comp_refs = val_comp['translation'].tolist()\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "for i in range(0, len(comp_trans), 4):\n",
    "    batch = [PREFIX + t for t in comp_trans[i:i+4]]\n",
    "    enc = tokenizer(batch, max_length=MAX_SOURCE, truncation=True,\n",
    "                    padding=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **enc, max_length=MAX_TARGET, num_beams=5,\n",
    "            length_penalty=1.0, no_repeat_ngram_size=3, early_stopping=True,\n",
    "        )\n",
    "    preds.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "\n",
    "scores = score_preds(preds, comp_refs)\n",
    "print(f'ByT5 final: BLEU={scores[\"bleu\"]:.2f}  chrF++={scores[\"chrf\"]:.2f}  geo_mean={scores[\"geo_mean\"]:.4f}')\n",
    "\n",
    "for j in range(min(5, len(preds))):\n",
    "    print(f'\\n[{j}] Src: {comp_trans[j][:120]}...')\n",
    "    print(f'    Out: {preds[j][:250]}')\n",
    "    print(f'    Ref: {comp_refs[j][:250]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_df is not None:\n",
    "    test_trans = test_df['transliteration'].tolist()\n",
    "    test_preds = []\n",
    "    for i in range(0, len(test_trans), 2):\n",
    "        batch = [PREFIX + t for t in test_trans[i:i+2]]\n",
    "        enc = tokenizer(batch, max_length=MAX_SOURCE, truncation=True,\n",
    "                        padding=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **enc, max_length=MAX_TARGET, num_beams=5,\n",
    "                length_penalty=1.0, no_repeat_ngram_size=3, early_stopping=True,\n",
    "            )\n",
    "        test_preds.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    \n",
    "    submission = pd.DataFrame({'id': test_df['id'], 'translation': test_preds})\n",
    "    submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "    print(f'Submission saved ({len(submission)} rows)')\n",
    "    \n",
    "    for j in range(len(test_preds)):\n",
    "        print(f'\\n[{j}] {test_trans[j][:100]}...')\n",
    "        print(f'    ByT5: {test_preds[j][:300]}')\n",
    "        if nllb_test_df is not None:\n",
    "            print(f'    NLLB: {nllb_test_df.iloc[j][\"nllb_translation\"][:300]}')\n",
    "else:\n",
    "    print('No test.csv found')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}