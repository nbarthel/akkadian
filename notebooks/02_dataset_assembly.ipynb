{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Dataset Assembly Overview\n",
    "\n",
    "This notebook documents the assembled dataset for the **Deep Past Challenge** (Akkadian-to-English translation).\n",
    "\n",
    "The data pipeline merged multiple sources (HuggingFace datasets, Kaggle competition data, lexicons, and OARE sentences), deduplicated them, and produced train/val/test splits stored as Parquet files under `data/processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "PROCESSED = Path(\"../data/processed\")\n",
    "STATS_PATH = PROCESSED / \"stats.json\"\n",
    "\n",
    "# Add project root so we can import src modules\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline stats if available, otherwise compute from parquet\n",
    "if STATS_PATH.exists():\n",
    "    with open(STATS_PATH) as f:\n",
    "        stats = json.load(f)\n",
    "    print(\"Loaded stats.json\")\n",
    "    print(json.dumps(stats, indent=2))\n",
    "else:\n",
    "    print(\"stats.json not found; will compute from all_data.parquet\")\n",
    "    stats = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full assembled dataset\n",
    "df = pd.read_parquet(PROCESSED / \"all_data.parquet\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "---\n",
    "## Source Inventory\n",
    "\n",
    "Each row is tagged with a `source` field indicating where the pair originated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_counts = df[\"source\"].value_counts()\n",
    "print(\"Rows per source:\\n\")\n",
    "print(source_counts.to_string())\n",
    "print(f\"\\nTotal: {source_counts.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "source_counts.sort_values().plot.barh(ax=ax, color=\"steelblue\")\n",
    "ax.set_xlabel(\"Number of rows\")\n",
    "ax.set_title(\"Rows per data source\")\n",
    "for i, v in enumerate(source_counts.sort_values()):\n",
    "    ax.text(v + 200, i, f\"{v:,}\", va=\"center\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "---\n",
    "## Normalization Examples\n",
    "\n",
    "The pipeline applies `normalize_transliteration()` to every transliteration. This function:\n",
    "1. Converts ASCII diacritics to Unicode (`sz` -> `s`, `s,` -> `s`, `t,` -> `t`)\n",
    "2. Subscripts digits on syllables (`du3` -> `du3`)\n",
    "3. Lowercases determinative braces (`{D}` -> `{d}`)\n",
    "4. Applies NFC Unicode normalization\n",
    "5. Normalizes whitespace\n",
    "\n",
    "Below we show 5 sample transliterations from the dataset, plus before/after normalization on a few raw strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 5 example transliterations from the assembled data\n",
    "samples = df.sample(5, random_state=42)[[\"transliteration\", \"translation\", \"source\"]]\n",
    "for i, row in samples.iterrows():\n",
    "    print(f\"[{row['source']}]\")\n",
    "    print(f\"  AKK: {row['transliteration'][:120]}\")\n",
    "    print(f\"  ENG: {row['translation'][:120]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.normalize import normalize_transliteration\n",
    "\n",
    "# Demonstrate normalization on raw strings\n",
    "raw_examples = [\n",
    "    \"a-na {D}EN.LIL2 qi2-bi2-ma\",\n",
    "    \"KISZIB szu-ta-mu-zi\",\n",
    "    \"i-na UGU s,i-ba-at {KI}ba-bi-lim\",\n",
    "    \"um-ma sza-lim-a-szur3-ma\",\n",
    "    \"1 GU2 AN.NA t,a-ab\",\n",
    "]\n",
    "\n",
    "print(f\"{'Raw':<45} {'Normalized'}\")\n",
    "print(\"-\" * 90)\n",
    "for raw in raw_examples:\n",
    "    normed = normalize_transliteration(raw)\n",
    "    print(f\"{raw:<45} {normed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "---\n",
    "## Dialect / Genre Distribution\n",
    "\n",
    "Most rows come from general HuggingFace corpora without dialect metadata. The competition data and OARE sentences are tagged as **Old Assyrian**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Dialect distribution\n",
    "dialect_counts = df[\"dialect\"].value_counts()\n",
    "dialect_counts.plot.bar(ax=axes[0], color=[\"#7cafc2\", \"#d28445\"])\n",
    "axes[0].set_title(\"Dialect distribution\")\n",
    "axes[0].set_ylabel(\"Rows\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=0)\n",
    "for j, v in enumerate(dialect_counts):\n",
    "    axes[0].text(j, v + 500, f\"{v:,}\", ha=\"center\", fontsize=9)\n",
    "\n",
    "# Genre distribution\n",
    "genre_counts = df[\"genre\"].value_counts()\n",
    "genre_counts.plot.bar(ax=axes[1], color=[\"#7cafc2\", \"#d28445\"])\n",
    "axes[1].set_title(\"Genre distribution\")\n",
    "axes[1].set_ylabel(\"Rows\")\n",
    "axes[1].tick_params(axis=\"x\", rotation=0)\n",
    "for j, v in enumerate(genre_counts):\n",
    "    axes[1].text(j, v + 500, f\"{v:,}\", ha=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation: source vs dialect\n",
    "print(\"Source x Dialect cross-tab:\\n\")\n",
    "ct = pd.crosstab(df[\"source\"], df[\"dialect\"])\n",
    "print(ct.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "---\n",
    "## Quality Tier Breakdown\n",
    "\n",
    "- **gold** — parallel transliteration-translation pairs from curated sources\n",
    "- **lexicon** — single-word or phrase-level entries from eBL dictionary / OA lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_counts = df[\"quality\"].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "colors = [\"#7cafc2\", \"#d28445\"]\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    quality_counts,\n",
    "    labels=quality_counts.index,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=colors,\n",
    "    startangle=90,\n",
    ")\n",
    "ax.set_title(\"Quality tier breakdown\")\n",
    "for t in autotexts:\n",
    "    t.set_fontsize(11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(quality_counts.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset Statistics\n",
    "\n",
    "Train/val/test split sizes, average lengths, and vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load splits\n",
    "train_df = pd.read_parquet(PROCESSED / \"train.parquet\")\n",
    "val_df = pd.read_parquet(PROCESSED / \"val.parquet\")\n",
    "test_df = pd.read_parquet(PROCESSED / \"test.parquet\")\n",
    "val_comp = pd.read_parquet(PROCESSED / \"val_competition.parquet\")\n",
    "\n",
    "print(\"Split sizes:\")\n",
    "print(f\"  train:            {len(train_df):>8,}\")\n",
    "print(f\"  val:              {len(val_df):>8,}\")\n",
    "print(f\"  test:             {len(test_df):>8,}\")\n",
    "print(f\"  val_competition:  {len(val_comp):>8,} (Old Assyrian kaggle-source only)\")\n",
    "print(f\"  total:            {len(train_df) + len(val_df) + len(test_df):>8,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average lengths (whitespace-split tokens)\n",
    "df[\"translit_len\"] = df[\"transliteration\"].str.split().str.len()\n",
    "df[\"transl_len\"] = df[\"translation\"].fillna(\"\").str.split().str.len()\n",
    "\n",
    "print(\"Transliteration length (whitespace tokens):\")\n",
    "print(f\"  mean:   {df['translit_len'].mean():.1f}\")\n",
    "print(f\"  median: {df['translit_len'].median():.0f}\")\n",
    "print(f\"  max:    {df['translit_len'].max()}\")\n",
    "print()\n",
    "print(\"Translation length (whitespace tokens):\")\n",
    "print(f\"  mean:   {df['transl_len'].mean():.1f}\")\n",
    "print(f\"  median: {df['transl_len'].median():.0f}\")\n",
    "print(f\"  max:    {df['transl_len'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(df[\"translit_len\"].clip(upper=100), bins=50, color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[0].set_title(\"Transliteration length distribution\")\n",
    "axes[0].set_xlabel(\"Tokens (clipped at 100)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1].hist(df[\"transl_len\"].clip(upper=100), bins=50, color=\"#d28445\", edgecolor=\"white\")\n",
    "axes[1].set_title(\"Translation length distribution\")\n",
    "axes[1].set_xlabel(\"Tokens (clipped at 100)\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size (unique whitespace-split transliteration tokens)\n",
    "all_tokens = df[\"transliteration\"].str.split().explode()\n",
    "vocab_size = all_tokens.nunique()\n",
    "print(f\"Unique transliteration tokens (whitespace-split): {vocab_size:,}\")\n",
    "print(f\"Total transliteration tokens: {len(all_tokens):,}\")\n",
    "\n",
    "# Most common tokens\n",
    "print(\"\\nTop 20 most frequent transliteration tokens:\")\n",
    "print(all_tokens.value_counts().head(20).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparison to Baseline\n",
    "\n",
    "The original baseline model was trained on **1,561** parallel pairs from the Kaggle competition `train.csv`. After assembling data from HuggingFace (cipher-ling and phucthaiv02), the eBL dictionary, the OA lexicon, and OARE sentences, we now have **161,518** deduplicated rows -- a **~103x increase** in training data.\n",
    "\n",
    "| Metric | Baseline | Assembled |\n",
    "|--------|----------|-----------|\n",
    "| Total rows | 1,561 | 161,518 |\n",
    "| Sources | 1 (Kaggle) | 7 |\n",
    "| Dialects tagged | 1 | 2 (old_assyrian, unknown) |\n",
    "| Quality tiers | 1 | 2 (gold, lexicon) |\n",
    "| Train split | ~1,404 (90%) | 145,366 |\n",
    "| Val split | ~157 (10%) | 8,076 |\n",
    "| Competition val | -- | 88 (OA-only) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "bars = ax.bar(\n",
    "    [\"Baseline\\n(Kaggle only)\", \"Assembled\\n(all sources)\"],\n",
    "    [1561, len(df)],\n",
    "    color=[\"#d28445\", \"#7cafc2\"],\n",
    "    edgecolor=\"white\",\n",
    "    width=0.5,\n",
    ")\n",
    "ax.set_ylabel(\"Number of parallel pairs\")\n",
    "ax.set_title(\"Training data: baseline vs assembled\")\n",
    "ax.bar_label(bars, fmt=\"{:,.0f}\", fontsize=11, padding=3)\n",
    "ax.set_ylim(0, len(df) * 1.15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ratio = len(df) / 1561\n",
    "print(f\"Assembled dataset is {ratio:.0f}x larger than the baseline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next steps:**\n",
    "- Train ByT5-small on the full assembled dataset and compare against the 1,561-pair baseline\n",
    "- Experiment with filtering to gold-quality-only vs including lexicon entries\n",
    "- Evaluate on the `val_competition` split (88 Old Assyrian pairs) for competition-relevant performance\n",
    "- Scale to ByT5-base/large once data pipeline is validated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
